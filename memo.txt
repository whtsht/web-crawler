 HTML = { baseURI, Document }

 Phase 1 downloadContent stream uri -> list doc
 [URI] =>
 if uri is content
 - saveFile(content)
 -> [HTML]

 Phase 2 extractLinkedElement stream HTML -> stream LinkedElement
 [Document] -> [LinkedElement]

 Phase 3 rewriteLinkedElement stream LinkedElement -> stream uri
 [LinkedElement] =>
 LinkedElement.applyNewUri(baseUri)
 -> [URI]

 Phase 4 saveDocuments stream doc -> ()
 [Document] => saveFile(doc)

crawlingWithDepth
iterate 0:depth 
    var docs = (1)(uri)
    uri  = (2->3)(docs)
    saveDocuments(docs)

var docs = (1)(uri)
saveDocuments(docs)

Behavior
- depth = 1:
    Level 1:
        HTML     : Downloaded
        Contents : Downloaded
    Level 2:
        HTML     : Downloaded but hyperlinks are maybe broken
        Contents : Not Downloaded

- depth = 2:
    Level 1:
        HTML     : Downloaded
        Contents : Downloaded
    Level 2:
        HTML     : Downloaded
        Contents : Downloaded
    Level 3:
        HTML     : Downloaded but hyperlinks are maybe broken
        Contents : Not Downloaded

- depth = 3:
...


depth 1 => 2~4s
depth 2 => 30s
depth 3 => NAN
